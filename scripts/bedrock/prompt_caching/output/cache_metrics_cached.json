{
  "model_id": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
  "timestamp": "2026-01-22T11:37:28.272410",
  "summary": {
    "total_requests": 3,
    "total_cost": 0.1202937,
    "avg_latency_ms": 32199.57359631856,
    "cache_hit_rate": 1.0
  },
  "responses": [
    {
      "metric": {
        "timestamp": "2026-01-22 11:36:26.705817",
        "latency_ms": 36042.73009300232,
        "input_tokens": 17,
        "cache_creation_tokens": 1602,
        "cache_read_tokens": 0,
        "output_tokens": 2827,
        "total_tokens": 4446,
        "cost_usd": 0.0484635,
        "used_cache": true,
        "user_message": "How do I implement proper error handling in Lambda functions?"
      },
      "response": "# Implementing Proper Error Handling in AWS Lambda Functions\n\nError handling is critical in serverless applications to ensure reliability, maintainability, and observability. Here's a comprehensive approach to implementing proper error handling in Lambda functions:\n\n## 1. Structured Error Handling Framework\n\n```python\nimport json\nimport logging\nimport traceback\nfrom enum import Enum\nfrom typing import Dict, Any, Optional\n\n# Configure logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nclass ErrorType(Enum):\n    VALIDATION_ERROR = \"ValidationError\"\n    DEPENDENCY_ERROR = \"DependencyError\"\n    BUSINESS_LOGIC_ERROR = \"BusinessLogicError\"\n    PERMISSION_ERROR = \"PermissionError\"\n    CONFIGURATION_ERROR = \"ConfigurationError\"\n    UNEXPECTED_ERROR = \"UnexpectedError\"\n\nclass CustomError(Exception):\n    def __init__(self, error_type: ErrorType, message: str, details: Optional[Dict[str, Any]] = None):\n        self.error_type = error_type\n        self.message = message\n        self.details = details or {}\n        super().__init__(self.message)\n\ndef lambda_handler(event, context):\n    try:\n        # Parse and validate input\n        validate_input(event)\n        \n        # Process the request\n        result = process_request(event)\n        \n        # Return successful response\n        return {\n            'statusCode': 200,\n            'body': json.dumps(result)\n        }\n    \n    except CustomError as e:\n        # Handle known error types\n        error_response = handle_custom_error(e)\n        return error_response\n    \n    except Exception as e:\n        # Handle unexpected errors\n        return handle_unexpected_error(e, context)\n\ndef handle_custom_error(error: CustomError) -> Dict[str, Any]:\n    \"\"\"Handle custom application errors with appropriate status codes and logging\"\"\"\n    \n    # Map error types to HTTP status codes\n    status_code_map = {\n        ErrorType.VALIDATION_ERROR: 400,\n        ErrorType.PERMISSION_ERROR: 403,\n        ErrorType.BUSINESS_LOGIC_ERROR: 422,\n        ErrorType.DEPENDENCY_ERROR: 502,\n        ErrorType.CONFIGURATION_ERROR: 500,\n        ErrorType.UNEXPECTED_ERROR: 500\n    }\n    \n    status_code = status_code_map.get(error.error_type, 500)\n    \n    # Log the error with appropriate level based on severity\n    if status_code >= 500:\n        logger.error(f\"{error.error_type.value}: {error.message}\", \n                    extra={\"error_details\": error.details})\n    else:\n        logger.warning(f\"{error.error_type.value}: {error.message}\", \n                      extra={\"error_details\": error.details})\n    \n    # Return formatted error response\n    return {\n        'statusCode': status_code,\n        'body': json.dumps({\n            'error': error.error_type.value,\n            'message': error.message,\n            'requestId': context.aws_request_id\n        })\n    }\n\ndef handle_unexpected_error(error: Exception, context) -> Dict[str, Any]:\n    \"\"\"Handle unexpected errors with proper logging and generic user message\"\"\"\n    \n    # Log the full error details including stack trace\n    logger.error(\n        f\"Unexpected error: {str(error)}\",\n        extra={\n            \"traceback\": traceback.format_exc(),\n            \"request_id\": context.aws_request_id,\n            \"function_name\": context.function_name,\n            \"remaining_time_ms\": context.get_remaining_time_in_millis()\n        }\n    )\n    \n    # Return a generic error message to avoid leaking implementation details\n    return {\n        'statusCode': 500,\n        'body': json.dumps({\n            'error': 'InternalServerError',\n            'message': 'An unexpected error occurred',\n            'requestId': context.aws_request_id\n        })\n    }\n```\n\n## 2. Input Validation\n\n```python\nfrom jsonschema import validate, ValidationError as JsonSchemaValidationError\n\n# Define schema for input validation\nEVENT_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"userId\", \"action\"],\n    \"properties\": {\n        \"userId\": {\"type\": \"string\", \"minLength\": 1},\n        \"action\": {\"type\": \"string\", \"enum\": [\"create\", \"update\", \"delete\"]},\n        \"payload\": {\"type\": \"object\"}\n    }\n}\n\ndef validate_input(event):\n    \"\"\"Validate the input event against schema\"\"\"\n    try:\n        validate(instance=event, schema=EVENT_SCHEMA)\n    except JsonSchemaValidationError as e:\n        raise CustomError(\n            ErrorType.VALIDATION_ERROR,\n            f\"Invalid input: {e.message}\",\n            {\"path\": e.path, \"schema_path\": e.schema_path}\n        )\n    \n    # Additional business validation logic\n    if event[\"action\"] == \"update\" and \"id\" not in event.get(\"payload\", {}):\n        raise CustomError(\n            ErrorType.VALIDATION_ERROR,\n            \"Update action requires an id in the payload\",\n            {\"action\": event[\"action\"]}\n        )\n```\n\n## 3. Handling AWS Service Errors\n\n```python\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('Users')\n\ndef get_user(user_id):\n    \"\"\"Retrieve user from DynamoDB with proper error handling\"\"\"\n    try:\n        response = table.get_item(Key={'userId': user_id})\n        return response.get('Item')\n    except ClientError as e:\n        error_code = e.response['Error']['Code']\n        \n        if error_code == 'ResourceNotFoundException':\n            raise CustomError(\n                ErrorType.DEPENDENCY_ERROR,\n                f\"The requested table does not exist\",\n                {\"table\": \"Users\"}\n            )\n        elif error_code == 'ProvisionedThroughputExceededException':\n            raise CustomError(\n                ErrorType.DEPENDENCY_ERROR,\n                \"DynamoDB throughput exceeded\",\n                {\"service\": \"DynamoDB\", \"operation\": \"get_item\"}\n            )\n        elif error_code == 'AccessDeniedException':\n            raise CustomError(\n                ErrorType.PERMISSION_ERROR,\n                \"Insufficient permissions to access DynamoDB\",\n                {\"service\": \"DynamoDB\", \"operation\": \"get_item\"}\n            )\n        else:\n            raise CustomError(\n                ErrorType.DEPENDENCY_ERROR,\n                f\"DynamoDB error: {error_code}\",\n                {\"service\": \"DynamoDB\", \"error_code\": error_code}\n            )\n```\n\n## 4. Implementing Retry Logic\n\n```python\nimport time\nimport random\nfrom functools import wraps\n\ndef retry_with_backoff(max_retries=3, base_delay=1, max_delay=30, retryable_exceptions=()):\n    \"\"\"Decorator for retrying functions with exponential backoff\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            for attempt in range(max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except retryable_exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_retries:\n                        logger.warning(f\"Max retries ({max_retries}) reached for {func.__name__}\")\n                        raise\n                    \n                    # Calculate backoff delay with jitter\n                    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n                    \n                    logger.info(f\"Retry {attempt+1}/{max_retries} for {func.__name__} after {delay:.2f}s due to {str(e)}\")\n                    time.sleep(delay)\n            \n            # This should never happen, but just in case\n            raise last_exception\n        return wrapper\n    return decorator\n\n# Example usage\n@retry_with_backoff(\n    max_retries=3, \n    retryable_exceptions=(\n        ClientError,  # Retry on specific boto3 errors\n    )\n)\ndef save_to_dynamodb(item):\n    return table.put_item(Item=item)\n```\n\n## 5. Dead Letter Queues for Asynchronous Invocations\n\nFor asynchronous Lambda invocations, configure a Dead Letter Queue (DLQ) to capture failed events:\n\n```python\n# This would be part of your AWS CDK or CloudFormation template\nimport aws_cdk as cdk\nfrom aws_cdk import (\n    aws_lambda as lambda_,\n    aws_sqs as sqs,\n)\n\nclass ErrorHandlingStack(cdk.Stack):\n    def __init__(self, scope, id, **kwargs):\n        super().__init__(scope, id, **kwargs)\n        \n        # Create a Dead Letter Queue\n        dlq = sqs.Queue(\n            self, \"ProcessingDLQ\",\n            retention_period=cdk.Duration.days(14),\n            visibility_timeout=cdk.Duration.seconds(30)\n        )\n        \n        # Create Lambda function with DLQ configuration\n        lambda_function = lambda_.Function(\n            self, \"ProcessingFunction\",\n            runtime=lambda_.Runtime.PYTHON_3_9,\n            handler=\"index.lambda_handler\",\n            code=lambda_.Code.from_asset(\"lambda\"),\n            dead_letter_queue=dlq,  # Configure the DLQ\n            retry_attempts=2,       # Number of retries before sending to DLQ\n        )\n```\n\n## 6. Monitoring and Alerting\n\n```python\n# Add custom metrics for error tracking\ndef record_error_metric(error_type):\n    \"\"\"Record custom CloudWatch metric for error tracking\"\"\"\n    try:\n        cloudwatch = boto3.client('cloudwatch')\n        cloudwatch.put_metric_data(\n            Namespace='MyApplication',\n            MetricData=[\n                {\n                    'MetricName': 'ErrorCount',\n                    'Dimensions': [\n                        {\n                            'Name': 'ErrorType',\n                            'Value': error_type\n                        },\n                        {\n                            'Name': 'FunctionName',\n                            'Value': context.function_name\n                        }\n                    ],\n                    'Value': 1.0,\n                    'Unit': 'Count'\n                }\n            ]\n        )\n    except Exception as e:\n        # Don't let metric recording failure affect the main flow\n        logger.warning(f\"Failed to record error metric: {str(e)}\")\n```\n\n## 7. Best Practices Summary\n\n1. **Categorize errors** into different types (validation, dependency, business logic, etc.)\n2. **Return appropriate HTTP status codes** based on error type\n3. **Include request IDs** in error responses for correlation\n4. **Log detailed error information** but return generic messages to users\n5. **Implement retries with exponential backoff** for transient failures\n6. **Use Dead Letter Queues** for asynchronous invocations\n7. **Add custom metrics** for error tracking and alerting\n8. **Validate input** early to fail fast\n9. **Handle AWS service errors** specifically based on error codes\n10. **Use structured logging** with context for better troubleshooting\n\nBy implementing these error handling patterns, your Lambda functions will be more resilient, easier to debug, and provide better user experience when things go wrong."
    },
    {
      "metric": {
        "timestamp": "2026-01-22 11:36:53.499711",
        "latency_ms": 26288.667917251587,
        "input_tokens": 17,
        "cache_creation_tokens": 0,
        "cache_read_tokens": 1602,
        "output_tokens": 2034,
        "total_tokens": 3653,
        "cost_usd": 0.031041599999999996,
        "used_cache": true,
        "user_message": "What are the best practices for async operations in Python?"
      },
      "response": "# Best Practices for Asynchronous Operations in Python\n\n## Core Concepts and Patterns\n\n### 1. Using `asyncio` Effectively\n\n```python\nimport asyncio\n\nasync def fetch_data(id):\n    # Simulate network I/O\n    await asyncio.sleep(1)\n    return {\"id\": id, \"data\": f\"Result for {id}\"}\n\nasync def process_all():\n    # Create tasks for concurrent execution\n    tasks = [fetch_data(i) for i in range(10)]\n    \n    # Wait for all tasks to complete\n    results = await asyncio.gather(*tasks)\n    \n    return results\n\n# Run the async function\nasync def main():\n    results = await process_all()\n    print(f\"Processed {len(results)} items\")\n\n# Python 3.7+\nasyncio.run(main())\n```\n\n### 2. Task Management\n\n```python\nimport asyncio\nimport time\n\nasync def background_task(name, duration):\n    try:\n        print(f\"Task {name} started\")\n        await asyncio.sleep(duration)\n        print(f\"Task {name} completed after {duration}s\")\n        return f\"Result from {name}\"\n    except asyncio.CancelledError:\n        print(f\"Task {name} was cancelled\")\n        raise\n\nasync def main():\n    # Create tasks\n    task1 = asyncio.create_task(background_task(\"A\", 2))\n    task2 = asyncio.create_task(background_task(\"B\", 3))\n    \n    # Wait for first task to complete\n    result = await task1\n    print(f\"Got result: {result}\")\n    \n    # Cancel the second task if needed\n    if not task2.done():\n        task2.cancel()\n        try:\n            await task2\n        except asyncio.CancelledError:\n            print(\"Task B cancelled successfully\")\n\nasyncio.run(main())\n```\n\n### 3. Error Handling in Async Code\n\n```python\nimport asyncio\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def risky_operation(item):\n    try:\n        if item % 3 == 0:\n            raise ValueError(f\"Error processing item {item}\")\n        await asyncio.sleep(0.5)\n        return f\"Processed {item}\"\n    except Exception as e:\n        logger.error(f\"Operation failed: {e}\")\n        return None\n\nasync def process_with_error_handling():\n    tasks = [risky_operation(i) for i in range(10)]\n    \n    # Option 1: Using gather with return_exceptions=True\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    \n    # Process results, handling exceptions\n    valid_results = [r for r in results if not isinstance(r, Exception) and r is not None]\n    \n    # Option 2: Using tasks with individual error handling\n    pending = [asyncio.create_task(risky_operation(i)) for i in range(10, 20)]\n    while pending:\n        done, pending = await asyncio.wait(\n            pending, \n            return_when=asyncio.FIRST_COMPLETED\n        )\n        \n        for task in done:\n            try:\n                result = task.result()\n                if result:\n                    logger.info(f\"Task completed: {result}\")\n            except Exception as e:\n                logger.error(f\"Task failed with: {e}\")\n```\n\n## Advanced Patterns\n\n### 1. Semaphores for Concurrency Control\n\n```python\nimport asyncio\nimport aiohttp\n\nasync def fetch_url(session, url, semaphore):\n    async with semaphore:  # Limit concurrent requests\n        print(f\"Fetching {url}\")\n        async with session.get(url) as response:\n            return await response.text()\n\nasync def main():\n    # Limit to 5 concurrent requests\n    semaphore = asyncio.Semaphore(5)\n    \n    urls = [f\"https://example.com/{i}\" for i in range(20)]\n    \n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url, semaphore) for url in urls]\n        results = await asyncio.gather(*tasks)\n        \n    print(f\"Fetched {len(results)} URLs\")\n\nasyncio.run(main())\n```\n\n### 2. Timeouts and Cancellation\n\n```python\nimport asyncio\n\nasync def operation_with_timeout(timeout_seconds):\n    try:\n        return await asyncio.wait_for(\n            slow_operation(), \n            timeout=timeout_seconds\n        )\n    except asyncio.TimeoutError:\n        print(f\"Operation timed out after {timeout_seconds} seconds\")\n        return None\n\nasync def slow_operation():\n    await asyncio.sleep(10)  # Simulating slow operation\n    return \"Operation completed\"\n\n# With graceful cancellation\nasync def with_graceful_cancellation():\n    try:\n        # Start a task\n        task = asyncio.create_task(slow_operation())\n        \n        # Wait for a while\n        await asyncio.sleep(2)\n        \n        # Cancel the task\n        task.cancel()\n        \n        # Wait for cancellation to complete\n        try:\n            await task\n        except asyncio.CancelledError:\n            print(\"Task was cancelled gracefully\")\n    finally:\n        # Cleanup code here\n        print(\"Cleanup completed\")\n```\n\n### 3. Producer-Consumer Pattern\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, items):\n    for i in range(items):\n        # Produce an item\n        item = f\"Item {i}\"\n        \n        # Put it in the queue\n        await queue.put(item)\n        print(f\"Produced {item}\")\n        \n        # Simulate variable production time\n        await asyncio.sleep(random.uniform(0.1, 0.5))\n    \n    # Signal the end of production\n    await queue.put(None)\n\nasync def consumer(queue, name):\n    while True:\n        # Get an item from the queue\n        item = await queue.get()\n        \n        # Check for end signal\n        if item is None:\n            print(f\"Consumer {name} shutting down\")\n            queue.task_done()\n            break\n        \n        # Process the item\n        print(f\"Consumer {name} processing {item}\")\n        await asyncio.sleep(random.uniform(0.2, 0.7))\n        \n        # Mark task as done\n        queue.task_done()\n\nasync def main():\n    # Create a queue\n    queue = asyncio.Queue(maxsize=5)  # Bounded queue\n    \n    # Create producer and consumers\n    producer_task = asyncio.create_task(producer(queue, 20))\n    consumers = [\n        asyncio.create_task(consumer(queue, i))\n        for i in range(3)\n    ]\n    \n    # Wait for producer to finish\n    await producer_task\n    \n    # Wait for all consumers to process all items\n    await queue.join()\n    \n    # Send shutdown signal to all consumers\n    for _ in range(len(consumers)):\n        await queue.put(None)\n    \n    # Wait for all consumers to shut down\n    await asyncio.gather(*consumers)\n\nasyncio.run(main())\n```\n\n## Best Practices\n\n1. **Use `async` with I/O-bound operations**: Network requests, file operations, database queries\n\n2. **Avoid CPU-bound work in async functions**: Use `concurrent.futures` with `ProcessPoolExecutor` for CPU-intensive tasks\n\n3. **Don't mix blocking and non-blocking code**: Blocking calls will halt the event loop\n\n4. **Handle exceptions properly**: Unhandled exceptions in tasks can be lost if not properly awaited\n\n5. **Use proper cancellation patterns**: Always handle `CancelledError` for cleanup\n\n6. **Set timeouts for external operations**: Prevent hanging on slow external services\n\n7. **Use connection pooling**: For database and HTTP connections\n\n8. **Monitor and limit concurrency**: Too many concurrent operations can overwhelm resources\n\n9. **Use structured logging**: Include correlation IDs across async operations for traceability\n\n10. **Test async code thoroughly**: Including error paths and cancellation scenarios\n\nBy following these patterns and best practices, you can build robust, efficient asynchronous systems in Python that handle concurrency gracefully while maintaining readability and reliability."
    },
    {
      "metric": {
        "timestamp": "2026-01-22 11:37:28.272244",
        "latency_ms": 34267.32277870178,
        "input_tokens": 16,
        "cache_creation_tokens": 0,
        "cache_read_tokens": 1602,
        "output_tokens": 2684,
        "total_tokens": 4302,
        "cost_usd": 0.0407886,
        "used_cache": true,
        "user_message": "How can I optimize DynamoDB batch operations?"
      },
      "response": "# Optimizing DynamoDB Batch Operations\n\n## Best Practices for DynamoDB Batch Operations\n\n### 1. Batch Size Optimization\n\n```python\ndef optimized_batch_write(items, table_name, max_batch_size=25):\n    \"\"\"\n    Process items in optimized batches for DynamoDB.\n    \n    Args:\n        items: List of items to write\n        table_name: Target DynamoDB table name\n        max_batch_size: Maximum items per batch (DynamoDB limit is 25)\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Process in batches of max_batch_size\n    for i in range(0, len(items), max_batch_size):\n        batch_items = items[i:i + max_batch_size]\n        request_items = {\n            table_name: [{'PutRequest': {'Item': item}} for item in batch_items]\n        }\n        \n        response = dynamodb.batch_write_item(RequestItems=request_items)\n        \n        # Handle unprocessed items with exponential backoff\n        unprocessed_items = response.get('UnprocessedItems', {})\n        retry_count = 0\n        \n        while unprocessed_items and retry_count < 5:\n            time.sleep(min(2 ** retry_count * 0.1, 1))  # Exponential backoff\n            retry_count += 1\n            \n            retry_response = dynamodb.batch_write_item(RequestItems=unprocessed_items)\n            unprocessed_items = retry_response.get('UnprocessedItems', {})\n```\n\n### 2. Parallel Processing for Large Datasets\n\n```python\nimport concurrent.futures\n\ndef parallel_batch_write(all_items, table_name, max_workers=10, chunk_size=100):\n    \"\"\"\n    Process large datasets in parallel batches.\n    \n    Args:\n        all_items: Complete list of items to write\n        table_name: Target DynamoDB table name\n        max_workers: Maximum number of parallel workers\n        chunk_size: Items per worker chunk\n    \"\"\"\n    # Split items into chunks for parallel processing\n    chunks = [all_items[i:i + chunk_size] for i in range(0, len(all_items), chunk_size)]\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(optimized_batch_write, chunk, table_name) \n            for chunk in chunks\n        ]\n        \n        # Wait for all futures to complete\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                future.result()\n            except Exception as e:\n                print(f\"Error in batch operation: {e}\")\n```\n\n### 3. Consistent Batch Gets with Error Handling\n\n```python\ndef batch_get_with_retries(keys, table_name, max_retries=3):\n    \"\"\"\n    Perform batch get with automatic retries and handling for unprocessed keys.\n    \n    Args:\n        keys: List of key dictionaries to retrieve\n        table_name: Source DynamoDB table name\n        max_retries: Maximum retry attempts\n    \n    Returns:\n        List of retrieved items\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    \n    all_results = []\n    unprocessed_keys = keys\n    retry_count = 0\n    \n    while unprocessed_keys and retry_count <= max_retries:\n        if retry_count > 0:\n            # Apply exponential backoff\n            time.sleep(min(2 ** (retry_count - 1) * 0.1, 1))\n        \n        # Process in batches of 100 (DynamoDB limit)\n        batch_keys = unprocessed_keys[:100]\n        unprocessed_keys = unprocessed_keys[100:]\n        \n        response = dynamodb.batch_get_item(\n            RequestItems={\n                table_name: {\n                    'Keys': batch_keys,\n                    'ConsistentRead': True\n                }\n            }\n        )\n        \n        # Add results to our collection\n        if table_name in response.get('Responses', {}):\n            all_results.extend(response['Responses'][table_name])\n        \n        # Handle any unprocessed keys from this batch\n        if table_name in response.get('UnprocessedKeys', {}):\n            unprocessed_keys.extend(\n                response['UnprocessedKeys'][table_name].get('Keys', [])\n            )\n            retry_count += 1\n        \n    if unprocessed_keys:\n        raise Exception(f\"Failed to process {len(unprocessed_keys)} keys after {max_retries} retries\")\n        \n    return all_results\n```\n\n### 4. Cost and Throughput Optimization\n\n```python\ndef calculate_batch_write_consumed_capacity(items):\n    \"\"\"\n    Calculate the approximate consumed capacity for a batch write operation.\n    \n    Args:\n        items: List of items to be written\n        \n    Returns:\n        Estimated WCU consumption\n    \"\"\"\n    total_wcu = 0\n    \n    for item in items:\n        # Calculate item size in KB (approximate)\n        item_size_bytes = len(str(item).encode('utf-8'))\n        item_size_kb = item_size_bytes / 1024\n        \n        # Each write unit can handle 1KB of data\n        # Round up to the nearest KB\n        wcu_for_item = math.ceil(item_size_kb)\n        total_wcu += wcu_for_item\n    \n    return total_wcu\n\ndef throttle_aware_batch_write(items, table_name, target_wcu_utilization=0.8):\n    \"\"\"\n    Batch write with awareness of provisioned throughput.\n    \n    Args:\n        items: List of items to write\n        table_name: Target DynamoDB table name\n        target_wcu_utilization: Target utilization of provisioned WCUs (0.0-1.0)\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    table = dynamodb.Table(table_name)\n    table_info = table.meta.client.describe_table(TableName=table_name)\n    \n    # Get provisioned WCU (or on-demand if applicable)\n    provisioned_wcu = table_info['Table'].get('ProvisionedThroughput', {}).get('WriteCapacityUnits')\n    \n    if not provisioned_wcu:  # On-demand table\n        # Use a reasonable default batch size\n        return optimized_batch_write(items, table_name)\n    \n    # Calculate optimal batch size based on WCU\n    target_wcu = provisioned_wcu * target_wcu_utilization\n    \n    # Estimate average item size\n    avg_item_size = sum(len(str(item).encode('utf-8')) for item in items[:100]) / min(100, len(items)) / 1024\n    \n    # Calculate optimal batch size\n    optimal_batch_size = min(25, int(target_wcu / avg_item_size))\n    \n    # Use at least 1 item per batch\n    optimal_batch_size = max(1, optimal_batch_size)\n    \n    return optimized_batch_write(items, table_name, max_batch_size=optimal_batch_size)\n```\n\n### 5. Transactional Batch Operations\n\n```python\ndef transactional_write(items, table_name):\n    \"\"\"\n    Perform a transactional write for up to 25 items.\n    All operations succeed or fail together.\n    \n    Args:\n        items: List of items to write (max 25)\n        table_name: Target DynamoDB table name\n    \"\"\"\n    if len(items) > 25:\n        raise ValueError(\"TransactWriteItems supports a maximum of 25 items\")\n    \n    dynamodb = boto3.client('dynamodb')\n    \n    transact_items = []\n    for item in items:\n        transact_items.append({\n            'Put': {\n                'TableName': table_name,\n                'Item': item,\n                'ConditionExpression': 'attribute_not_exists(pk) OR attribute_exists(pk)'\n            }\n        })\n    \n    try:\n        response = dynamodb.transact_write_items(TransactItems=transact_items)\n        return True\n    except dynamodb.exceptions.TransactionCanceledException as e:\n        # Analyze which items caused the transaction to fail\n        cancellation_reasons = e.response.get('CancellationReasons', [])\n        failed_items = [\n            items[i] for i, reason in enumerate(cancellation_reasons)\n            if reason.get('Code') != 'None'\n        ]\n        raise Exception(f\"Transaction failed for items: {failed_items}\")\n```\n\n### 6. Monitoring and Metrics\n\n```python\ndef batch_operation_with_metrics(operation_func, *args, **kwargs):\n    \"\"\"\n    Wrapper to collect metrics on batch operations.\n    \n    Args:\n        operation_func: The batch operation function to monitor\n        *args, **kwargs: Arguments to pass to the operation function\n    \"\"\"\n    import time\n    import boto3\n    \n    cloudwatch = boto3.client('cloudwatch')\n    start_time = time.time()\n    \n    try:\n        result = operation_func(*args, **kwargs)\n        success = True\n    except Exception as e:\n        success = False\n        raise\n    finally:\n        duration = time.time() - start_time\n        \n        # Send metrics to CloudWatch\n        cloudwatch.put_metric_data(\n            Namespace='DynamoDBBatchOperations',\n            MetricData=[\n                {\n                    'MetricName': 'BatchOperationDuration',\n                    'Value': duration,\n                    'Unit': 'Seconds',\n                    'Dimensions': [\n                        {'Name': 'OperationType', 'Value': operation_func.__name__},\n                        {'Name': 'Success', 'Value': str(success)}\n                    ]\n                }\n            ]\n        )\n        \n    return result\n```\n\n## Implementation Tips\n\n1. **Partition Key Distribution**: Ensure your batch operations distribute evenly across partition keys to avoid hot partitions.\n\n2. **Item Size Awareness**: Remember that DynamoDB charges for the full item size on each update, so batch updates on large items consume more capacity.\n\n3. **Conditional Writes**: Use conditional expressions in batch operations to prevent overwriting data unexpectedly.\n\n4. **Error Handling**: Always implement proper error handling for unprocessed items, especially for mission-critical data.\n\n5. **Cost Monitoring**: Set up CloudWatch alarms to monitor consumed capacity and costs, particularly for large batch operations.\n\n6. **Testing**: Test batch operations with realistic data volumes in a staging environment before running in production.\n\nBy implementing these optimization strategies, you can significantly improve the efficiency, reliability, and cost-effectiveness of your DynamoDB batch operations."
    }
  ]
}