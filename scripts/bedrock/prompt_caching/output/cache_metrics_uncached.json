{
  "model_id": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
  "timestamp": "2026-01-22T11:35:48.651973",
  "summary": {
    "total_requests": 3,
    "total_cost": 0.132168,
    "avg_latency_ms": 35574.60069656372,
    "cache_hit_rate": 0.0
  },
  "responses": [
    {
      "metric": {
        "timestamp": "2026-01-22 11:34:40.694668",
        "latency_ms": 39774.55401420593,
        "input_tokens": 1619,
        "cache_creation_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 3027,
        "total_tokens": 4646,
        "cost_usd": 0.050262,
        "used_cache": false,
        "user_message": "How do I implement proper error handling in Lambda functions?"
      },
      "response": "# Implementing Proper Error Handling in AWS Lambda Functions\n\nEffective error handling is crucial for building reliable serverless applications. Here's a comprehensive approach to implementing proper error handling in AWS Lambda functions:\n\n## 1. Structured Error Handling Framework\n\n```python\nimport json\nimport logging\nimport traceback\nfrom enum import Enum\nfrom typing import Dict, Any, Optional\n\n# Configure logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\nclass ErrorType(Enum):\n    VALIDATION_ERROR = \"ValidationError\"\n    RESOURCE_NOT_FOUND = \"ResourceNotFound\"\n    DEPENDENCY_ERROR = \"DependencyError\"\n    PERMISSION_ERROR = \"PermissionError\"\n    THROTTLING_ERROR = \"ThrottlingError\"\n    INTERNAL_ERROR = \"InternalError\"\n\nclass CustomError(Exception):\n    def __init__(self, error_type: ErrorType, message: str, details: Optional[Dict[str, Any]] = None):\n        self.error_type = error_type\n        self.message = message\n        self.details = details or {}\n        super().__init__(self.message)\n\ndef lambda_handler(event, context):\n    try:\n        # Parse and validate input\n        validate_input(event)\n        \n        # Process the request\n        result = process_request(event)\n        \n        # Return successful response\n        return {\n            'statusCode': 200,\n            'body': json.dumps(result)\n        }\n    \n    except CustomError as e:\n        # Handle known application errors\n        error_response = handle_custom_error(e)\n        return error_response\n    \n    except Exception as e:\n        # Handle unexpected errors\n        logger.error(f\"Unexpected error: {str(e)}\")\n        logger.error(traceback.format_exc())\n        \n        return {\n            'statusCode': 500,\n            'body': json.dumps({\n                'error': 'InternalServerError',\n                'message': 'An unexpected error occurred',\n                'requestId': context.aws_request_id\n            })\n        }\n\ndef handle_custom_error(error: CustomError) -> Dict[str, Any]:\n    \"\"\"Map custom errors to appropriate HTTP responses\"\"\"\n    \n    error_mappings = {\n        ErrorType.VALIDATION_ERROR: 400,\n        ErrorType.RESOURCE_NOT_FOUND: 404,\n        ErrorType.PERMISSION_ERROR: 403,\n        ErrorType.DEPENDENCY_ERROR: 502,\n        ErrorType.THROTTLING_ERROR: 429,\n        ErrorType.INTERNAL_ERROR: 500\n    }\n    \n    status_code = error_mappings.get(error.error_type, 500)\n    \n    # Log the error with appropriate severity\n    if status_code >= 500:\n        logger.error(f\"{error.error_type.value}: {error.message}\")\n        if error.details:\n            logger.error(f\"Details: {json.dumps(error.details)}\")\n    else:\n        logger.warning(f\"{error.error_type.value}: {error.message}\")\n        if error.details:\n            logger.warning(f\"Details: {json.dumps(error.details)}\")\n    \n    return {\n        'statusCode': status_code,\n        'body': json.dumps({\n            'error': error.error_type.value,\n            'message': error.message,\n            'details': error.details\n        })\n    }\n```\n\n## 2. Handling AWS Service Errors\n\n```python\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('Users')\n\ndef get_user(user_id):\n    try:\n        response = table.get_item(Key={'userId': user_id})\n        \n        if 'Item' not in response:\n            raise CustomError(\n                ErrorType.RESOURCE_NOT_FOUND,\n                f\"User with ID {user_id} not found\"\n            )\n            \n        return response['Item']\n        \n    except ClientError as e:\n        error_code = e.response['Error']['Code']\n        \n        if error_code == 'ProvisionedThroughputExceededException':\n            raise CustomError(\n                ErrorType.THROTTLING_ERROR,\n                \"DynamoDB throughput exceeded\",\n                {\"service\": \"DynamoDB\", \"operation\": \"GetItem\"}\n            )\n        elif error_code == 'ResourceNotFoundException':\n            raise CustomError(\n                ErrorType.DEPENDENCY_ERROR,\n                \"DynamoDB table not found\",\n                {\"service\": \"DynamoDB\", \"table\": \"Users\"}\n            )\n        else:\n            raise CustomError(\n                ErrorType.DEPENDENCY_ERROR,\n                f\"DynamoDB error: {error_code}\",\n                {\"service\": \"DynamoDB\", \"errorCode\": error_code}\n            )\n```\n\n## 3. Implementing Retry Logic\n\n```python\nimport time\nimport random\nfrom functools import wraps\n\ndef retry_with_backoff(max_retries=3, base_delay=1, max_delay=60, retryable_exceptions=()):\n    \"\"\"Decorator for retrying functions with exponential backoff\"\"\"\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except retryable_exceptions as e:\n                    last_exception = e\n                    \n                    if attempt == max_retries - 1:\n                        # Last attempt failed, re-raise the exception\n                        raise\n                    \n                    # Calculate backoff delay with jitter\n                    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n                    \n                    logger.warning(\n                        f\"Attempt {attempt + 1}/{max_retries} failed with error: {str(e)}. \"\n                        f\"Retrying in {delay:.2f} seconds.\"\n                    )\n                    \n                    time.sleep(delay)\n            \n            # This should never happen, but just in case\n            raise last_exception if last_exception else Exception(\"Retry logic failed\")\n            \n        return wrapper\n    \n    return decorator\n\n# Example usage\n@retry_with_backoff(\n    max_retries=3,\n    retryable_exceptions=(\n        ClientError,  # Retry on AWS service errors\n        ConnectionError,  # Retry on network issues\n        TimeoutError  # Retry on timeouts\n    )\n)\ndef call_external_api(url):\n    # Implementation here\n    pass\n```\n\n## 4. Dead Letter Queues for Asynchronous Invocations\n\nFor asynchronous Lambda invocations, configure a Dead Letter Queue (DLQ) to capture failed events:\n\n```python\n# AWS CDK example (Python)\nfrom aws_cdk import (\n    aws_lambda as lambda_,\n    aws_sqs as sqs,\n    Stack\n)\nfrom constructs import Construct\n\nclass LambdaWithDlqStack(Stack):\n    def __init__(self, scope: Construct, id: str, **kwargs):\n        super().__init__(scope, id, **kwargs)\n        \n        # Create a Dead Letter Queue\n        dlq = sqs.Queue(\n            self, \"ProcessingDLQ\",\n            retention_period=Duration.days(14)\n        )\n        \n        # Create Lambda function with DLQ configuration\n        lambda_function = lambda_.Function(\n            self, \"ProcessingFunction\",\n            runtime=lambda_.Runtime.PYTHON_3_9,\n            handler=\"index.handler\",\n            code=lambda_.Code.from_asset(\"lambda\"),\n            dead_letter_queue=dlq,  # Configure the DLQ\n            max_retry_attempts=2\n        )\n```\n\n## 5. Input Validation\n\n```python\nfrom jsonschema import validate, ValidationError as JsonSchemaValidationError\n\n# Define schema for input validation\nuser_schema = {\n    \"type\": \"object\",\n    \"required\": [\"userId\", \"name\", \"email\"],\n    \"properties\": {\n        \"userId\": {\"type\": \"string\", \"pattern\": \"^[a-zA-Z0-9-]+$\"},\n        \"name\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 100},\n        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n        \"age\": {\"type\": \"integer\", \"minimum\": 0}\n    }\n}\n\ndef validate_input(event):\n    try:\n        # Extract the body if this is an API Gateway event\n        if 'body' in event:\n            if isinstance(event['body'], str):\n                body = json.loads(event['body'])\n            else:\n                body = event['body']\n        else:\n            body = event\n            \n        validate(instance=body, schema=user_schema)\n        return body\n        \n    except JsonSchemaValidationError as e:\n        path = '.'.join(str(p) for p in e.path)\n        raise CustomError(\n            ErrorType.VALIDATION_ERROR,\n            f\"Invalid input: {e.message}\",\n            {\"field\": path or \"unknown\"}\n        )\n    except json.JSONDecodeError:\n        raise CustomError(\n            ErrorType.VALIDATION_ERROR,\n            \"Invalid JSON in request body\"\n        )\n```\n\n## 6. Contextual Logging\n\n```python\ndef lambda_handler(event, context):\n    # Set up request context for all logs\n    request_context = {\n        'aws_request_id': context.aws_request_id,\n        'function_name': context.function_name,\n        'function_version': context.function_version,\n        'remaining_time_ms': context.get_remaining_time_in_millis()\n    }\n    \n    # Add correlation ID if present\n    if 'headers' in event and event['headers'] and 'X-Correlation-ID' in event['headers']:\n        request_context['correlation_id'] = event['headers']['X-Correlation-ID']\n    \n    logger.info(\"Request started\", extra={\"context\": request_context})\n    \n    try:\n        # Function implementation\n        result = process_request(event)\n        \n        logger.info(\"Request completed successfully\", extra={\"context\": request_context})\n        return result\n        \n    except Exception as e:\n        logger.error(\n            f\"Request failed: {str(e)}\",\n            extra={\"context\": request_context, \"error\": str(e)}\n        )\n        raise\n```\n\n## 7. Monitoring and Alerting\n\nSet up CloudWatch Alarms for:\n- Error rates\n- Duration thresholds\n- Throttling events\n- DLQ message counts\n\n```python\n# AWS CDK example\nfrom aws_cdk import (\n    aws_cloudwatch as cloudwatch,\n    aws_cloudwatch_actions as actions,\n    aws_sns as sns\n)\n\n# Create an SNS topic for alerts\nalert_topic = sns.Topic(self, \"LambdaErrorAlerts\")\n\n# Create an alarm for Lambda errors\nerror_alarm = cloudwatch.Alarm(\n    self, \"LambdaErrorAlarm\",\n    metric=lambda_function.metric_errors(),\n    threshold=1,\n    evaluation_periods=1,\n    alarm_description=\"Alarm if the Lambda function has any errors\",\n    treat_missing_data=cloudwatch.TreatMissingData.NOT_BREACHING\n)\n\n# Add action to the alarm\nerror_alarm.add_alarm_action(actions.SnsAction(alert_topic))\n```\n\n## 8. Best Practices Summary\n\n1. **Categorize errors** into operational vs. business logic errors\n2. **Use custom error classes** to standardize error handling\n3. **Implement proper HTTP status codes** for API responses\n4. **Add retry logic** with exponential backoff for transient failures\n5. **Configure Dead Letter Queues** for asynchronous invocations\n6. **Validate input** early to fail fast\n7. **Include context in logs** for easier troubleshooting\n8. **Set up monitoring and alerting** for error conditions\n9. **Return user-friendly error messages** but don't expose sensitive details\n10. **Log detailed error information** for debugging purposes\n\nBy implementing these patterns, you'll create more resilient Lambda functions that gracefully handle errors, provide meaningful feedback, and are easier to troubleshoot when issues occur."
    },
    {
      "metric": {
        "timestamp": "2026-01-22 11:35:17.739823",
        "latency_ms": 36539.76607322693,
        "input_tokens": 1619,
        "cache_creation_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 2423,
        "total_tokens": 4042,
        "cost_usd": 0.041201999999999996,
        "used_cache": false,
        "user_message": "What are the best practices for async operations in Python?"
      },
      "response": "# Best Practices for Asynchronous Operations in Python\n\n## Core Concepts and Patterns\n\n### 1. Using `asyncio` Effectively\n\n```python\nimport asyncio\n\nasync def fetch_data(id):\n    # Simulate network I/O\n    await asyncio.sleep(1)\n    return f\"Data for {id}\"\n\nasync def process_all():\n    # Create tasks for concurrent execution\n    tasks = [fetch_data(i) for i in range(10)]\n    \n    # Wait for all tasks to complete\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Run the async function\nasync def main():\n    results = await process_all()\n    print(f\"Processed {len(results)} items\")\n\n# In Python 3.7+\nasyncio.run(main())\n```\n\n### 2. Task Management\n\n```python\nimport asyncio\nimport time\n\nasync def main():\n    # Create and schedule tasks\n    task1 = asyncio.create_task(slow_operation(1))\n    task2 = asyncio.create_task(slow_operation(2))\n    \n    # Continue with other work while tasks run\n    print(\"Doing other work...\")\n    \n    # Wait for specific tasks with timeout\n    try:\n        result1 = await asyncio.wait_for(task1, timeout=2.0)\n        print(f\"Task 1 result: {result1}\")\n    except asyncio.TimeoutError:\n        print(\"Task 1 timed out\")\n        task1.cancel()\n    \n    # Wait for remaining tasks\n    result2 = await task2\n    print(f\"Task 2 result: {result2}\")\n\nasync def slow_operation(id):\n    await asyncio.sleep(id)\n    return f\"Result from operation {id}\"\n```\n\n### 3. Error Handling in Async Code\n\n```python\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nasync def fetch_with_retry(url, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            # Simulate HTTP request\n            if attempt < 2:  # Simulate failures on first attempts\n                raise ConnectionError(\"Connection failed\")\n            await asyncio.sleep(0.1)\n            return f\"Data from {url}\"\n        except Exception as e:\n            logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            await asyncio.sleep(0.5 * (2 ** attempt))  # Exponential backoff\n    \nasync def main():\n    try:\n        tasks = [\n            fetch_with_retry(\"api.example.com/users\"),\n            fetch_with_retry(\"api.example.com/products\")\n        ]\n        \n        # Process results as they complete\n        for task in asyncio.as_completed(tasks):\n            try:\n                result = await task\n                print(f\"Success: {result}\")\n            except Exception as e:\n                print(f\"Task failed: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n```\n\n## Working with External Libraries\n\n### 1. HTTP Requests with `aiohttp`\n\n```python\nimport asyncio\nimport aiohttp\nfrom typing import List, Dict, Any\n\nasync def fetch_url(session: aiohttp.ClientSession, url: str) -> Dict[str, Any]:\n    async with session.get(url) as response:\n        if response.status != 200:\n            response.raise_for_status()\n        return await response.json()\n\nasync def fetch_all_urls(urls: List[str]) -> List[Dict[str, Any]]:\n    # Use a single session for connection pooling\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        return await asyncio.gather(*tasks, return_exceptions=True)\n\n# Process results, handling exceptions\nasync def process_api_data():\n    urls = [\n        \"https://api.example.com/users\",\n        \"https://api.example.com/products\",\n        \"https://api.example.com/orders\"\n    ]\n    \n    results = await fetch_all_urls(urls)\n    \n    processed_data = []\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            print(f\"Error fetching {urls[i]}: {result}\")\n        else:\n            processed_data.append(result)\n    \n    return processed_data\n```\n\n### 2. Database Operations with `asyncpg`\n\n```python\nimport asyncio\nimport asyncpg\nfrom typing import List, Dict\n\nasync def get_db_connection():\n    return await asyncpg.connect(\n        user='postgres',\n        password='password',\n        database='mydatabase',\n        host='localhost'\n    )\n\nasync def fetch_users(conn, limit: int = 100) -> List[Dict]:\n    # Use prepared statements for better security and performance\n    stmt = await conn.prepare('SELECT id, name, email FROM users LIMIT $1')\n    rows = await stmt.fetch(limit)\n    return [dict(row) for row in rows]\n\nasync def update_user_status(conn, user_id: int, status: str) -> bool:\n    result = await conn.execute(\n        'UPDATE users SET status = $1 WHERE id = $2',\n        status, user_id\n    )\n    return 'UPDATE 1' in result\n\nasync def main():\n    # Connection pooling\n    pool = await asyncpg.create_pool(\n        user='postgres',\n        password='password',\n        database='mydatabase',\n        host='localhost',\n        min_size=5,\n        max_size=20\n    )\n    \n    async with pool.acquire() as conn:\n        # Transaction management\n        async with conn.transaction():\n            users = await fetch_users(conn, 10)\n            for user in users:\n                await update_user_status(conn, user['id'], 'active')\n```\n\n## Advanced Patterns\n\n### 1. Semaphores for Concurrency Control\n\n```python\nimport asyncio\nimport aiohttp\n\nasync def fetch_with_limit(url, semaphore):\n    async with semaphore:  # Limit concurrent requests\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                return await response.text()\n\nasync def main():\n    # Limit to 5 concurrent requests\n    semaphore = asyncio.Semaphore(5)\n    \n    urls = [f\"https://example.com/{i}\" for i in range(100)]\n    tasks = [fetch_with_limit(url, semaphore) for url in urls]\n    \n    results = await asyncio.gather(*tasks)\n    return results\n```\n\n### 2. Producer-Consumer Pattern\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, items):\n    for i in range(items):\n        # Produce item\n        item = random.randint(1, 100)\n        \n        # Put item in the queue\n        await queue.put(item)\n        print(f\"Produced: {item}\")\n        \n        # Simulate varying production time\n        await asyncio.sleep(random.uniform(0.1, 0.5))\n    \n    # Signal the end of production\n    await queue.put(None)\n\nasync def consumer(queue, id):\n    while True:\n        # Get item from queue\n        item = await queue.get()\n        \n        # Check for termination signal\n        if item is None:\n            print(f\"Consumer {id} shutting down\")\n            # Forward termination signal to other consumers\n            await queue.put(None)\n            break\n        \n        # Process item\n        print(f\"Consumer {id} processing: {item}\")\n        await asyncio.sleep(random.uniform(0.2, 0.8))\n        \n        # Mark task as done\n        queue.task_done()\n\nasync def main():\n    # Create a queue\n    queue = asyncio.Queue(maxsize=10)  # Bounded queue\n    \n    # Create producer and consumers\n    producer_task = asyncio.create_task(producer(queue, 20))\n    consumer_tasks = [\n        asyncio.create_task(consumer(queue, i))\n        for i in range(3)  # 3 consumers\n    ]\n    \n    # Wait for producer to finish\n    await producer_task\n    \n    # Wait for all consumers to finish\n    await asyncio.gather(*consumer_tasks)\n```\n\n## Best Practices Summary\n\n1. **Use `asyncio.gather` for parallel tasks** when you need to wait for all tasks to complete.\n\n2. **Use `asyncio.as_completed` for processing results** as they arrive rather than waiting for all.\n\n3. **Implement proper error handling** with try/except blocks around await expressions.\n\n4. **Use connection pooling** with databases and HTTP clients to avoid connection overhead.\n\n5. **Control concurrency** with semaphores to prevent overwhelming external services.\n\n6. **Avoid blocking the event loop** - never use synchronous I/O or CPU-intensive operations directly in async code.\n\n7. **Use `asyncio.TimeoutError` and cancellation** to prevent tasks from running indefinitely.\n\n8. **Structure code with clear async boundaries** - don't mix async and sync code without proper transitions.\n\n9. **Use task groups** (Python 3.11+) for cleaner task management:\n\n```python\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(some_coro())\n        task2 = tg.create_task(another_coro())\n        # All tasks will be awaited when exiting the context manager\n        # Any exception will propagate\n```\n\n10. **Consider using higher-level abstractions** like `asyncio.Queue` for producer-consumer patterns.\n\nBy following these practices, you'll build more efficient, scalable, and maintainable asynchronous Python applications."
    },
    {
      "metric": {
        "timestamp": "2026-01-22 11:35:48.651712",
        "latency_ms": 30409.4820022583,
        "input_tokens": 1618,
        "cache_creation_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 2390,
        "total_tokens": 4008,
        "cost_usd": 0.040704000000000004,
        "used_cache": false,
        "user_message": "How can I optimize DynamoDB batch operations?"
      },
      "response": "# Optimizing DynamoDB Batch Operations\n\n## Best Practices for DynamoDB Batch Operations\n\n### 1. Batch Size Optimization\n\n```python\ndef optimized_batch_write(items, table_name, max_batch_size=25):\n    \"\"\"\n    Process items in optimized batches for DynamoDB.\n    \n    Args:\n        items: List of items to write\n        table_name: Target DynamoDB table name\n        max_batch_size: Maximum items per batch (DynamoDB limit is 25)\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Process in batches of max_batch_size\n    for i in range(0, len(items), max_batch_size):\n        batch_items = items[i:i + max_batch_size]\n        request_items = {\n            table_name: [{'PutRequest': {'Item': item}} for item in batch_items]\n        }\n        \n        response = dynamodb.batch_write_item(RequestItems=request_items)\n        \n        # Handle unprocessed items with exponential backoff\n        unprocessed_items = response.get('UnprocessedItems', {})\n        retry_count = 0\n        \n        while unprocessed_items and retry_count < 5:\n            time.sleep((2 ** retry_count) * 0.1)  # Exponential backoff\n            retry_count += 1\n            \n            retry_response = dynamodb.batch_write_item(RequestItems=unprocessed_items)\n            unprocessed_items = retry_response.get('UnprocessedItems', {})\n            \n        if unprocessed_items:\n            raise Exception(f\"Failed to process items after retries: {unprocessed_items}\")\n```\n\n### 2. Parallel Processing for Large Datasets\n\n```python\nimport concurrent.futures\n\ndef parallel_batch_write(all_items, table_name, max_workers=5, chunk_size=100):\n    \"\"\"\n    Process large datasets in parallel batches.\n    \n    Args:\n        all_items: Complete list of items to write\n        table_name: Target DynamoDB table name\n        max_workers: Maximum number of parallel workers\n        chunk_size: Items per worker chunk\n    \"\"\"\n    # Split items into chunks for parallel processing\n    chunks = [all_items[i:i + chunk_size] for i in range(0, len(all_items), chunk_size)]\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(optimized_batch_write, chunk, table_name) \n            for chunk in chunks\n        ]\n        \n        # Wait for all futures to complete, raising any exceptions\n        for future in concurrent.futures.as_completed(futures):\n            future.result()  # This will raise any exceptions from the thread\n```\n\n### 3. Consistent Item Size for Better Throughput\n\n```python\ndef normalize_item_size(item, target_size_bytes=4000):\n    \"\"\"\n    Normalize item size to optimize throughput consistency.\n    Large size variations can cause throughput issues.\n    \n    Args:\n        item: The DynamoDB item\n        target_size_bytes: Target size in bytes\n    \"\"\"\n    # Calculate current approximate size\n    current_size = len(json.dumps(item).encode('utf-8'))\n    \n    if current_size < target_size_bytes:\n        # Pad item with a filler attribute if it's too small\n        padding_size = target_size_bytes - current_size - 25  # Account for attribute name and overhead\n        if padding_size > 0:\n            item['_padding'] = 'X' * padding_size\n    \n    return item\n```\n\n### 4. Efficient Batch Get Implementation\n\n```python\ndef efficient_batch_get(keys, table_name, projection_expression=None):\n    \"\"\"\n    Efficiently retrieve items in batches with automatic pagination.\n    \n    Args:\n        keys: List of key dictionaries\n        table_name: Target DynamoDB table\n        projection_expression: Optional attributes to retrieve\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    max_batch_size = 100  # DynamoDB limit for batch_get_item\n    results = []\n    \n    # Process in batches\n    for i in range(0, len(keys), max_batch_size):\n        batch_keys = keys[i:i + max_batch_size]\n        request_items = {\n            table_name: {\n                'Keys': batch_keys\n            }\n        }\n        \n        # Add projection if specified\n        if projection_expression:\n            request_items[table_name]['ProjectionExpression'] = projection_expression\n        \n        response = dynamodb.batch_get_item(RequestItems=request_items)\n        results.extend(response['Responses'].get(table_name, []))\n        \n        # Handle unprocessed keys with backoff\n        unprocessed_keys = response.get('UnprocessedKeys', {})\n        retry_count = 0\n        \n        while unprocessed_keys and retry_count < 5:\n            time.sleep((2 ** retry_count) * 0.1)\n            retry_count += 1\n            \n            retry_response = dynamodb.batch_get_item(RequestItems=unprocessed_keys)\n            results.extend(retry_response['Responses'].get(table_name, []))\n            unprocessed_keys = retry_response.get('UnprocessedKeys', {})\n    \n    return results\n```\n\n### 5. Cost and Performance Monitoring\n\n```python\ndef monitor_batch_operation(func):\n    \"\"\"\n    Decorator to monitor batch operation performance and cost.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        consumed_capacity = 0\n        \n        # Add ReturnConsumedCapacity to kwargs if it's a DynamoDB operation\n        if 'ReturnConsumedCapacity' not in kwargs:\n            kwargs['ReturnConsumedCapacity'] = 'TOTAL'\n        \n        result = func(*args, **kwargs)\n        \n        # Extract consumed capacity from result if available\n        if isinstance(result, dict) and 'ConsumedCapacity' in result:\n            consumed_capacity = result['ConsumedCapacity'].get('CapacityUnits', 0)\n        \n        duration = time.time() - start_time\n        \n        print(f\"Operation: {func.__name__}\")\n        print(f\"Duration: {duration:.2f} seconds\")\n        print(f\"Consumed Capacity: {consumed_capacity} units\")\n        \n        return result\n    \n    return wrapper\n```\n\n### 6. Handling Throttling with Adaptive Backoff\n\n```python\nclass AdaptiveBackoff:\n    def __init__(self, initial_units=5, max_units=500):\n        self.current_units = initial_units\n        self.max_units = max_units\n        self.success_streak = 0\n        self.failure_streak = 0\n    \n    def increase_capacity(self):\n        \"\"\"Increase capacity after successful operations\"\"\"\n        self.failure_streak = 0\n        self.success_streak += 1\n        \n        if self.success_streak >= 10:\n            self.current_units = min(self.current_units * 2, self.max_units)\n            self.success_streak = 0\n    \n    def decrease_capacity(self):\n        \"\"\"Decrease capacity after throttling\"\"\"\n        self.success_streak = 0\n        self.failure_streak += 1\n        \n        # Decrease more aggressively with more failures\n        if self.failure_streak >= 3:\n            self.current_units = max(1, self.current_units // 2)\n        else:\n            self.current_units = max(1, int(self.current_units * 0.8))\n    \n    def get_batch_size(self):\n        \"\"\"Get current recommended batch size\"\"\"\n        return min(25, max(1, self.current_units // 2))\n```\n\n### 7. Transaction vs. Batch Operations\n\n```python\ndef choose_write_operation(items, need_atomicity=False):\n    \"\"\"\n    Choose between batch write and transaction based on requirements.\n    \n    Args:\n        items: Items to write\n        need_atomicity: Whether all-or-nothing semantics are required\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    table = dynamodb.Table('MyTable')\n    \n    if need_atomicity:\n        # Use transactions when atomicity is required (all or nothing)\n        # More expensive but guarantees consistency\n        with table.meta.client.transact_write_items() as transact:\n            for item in items:\n                transact.put(\n                    TableName='MyTable',\n                    Item=item\n                )\n    else:\n        # Use batch operations when atomicity is not required\n        # More cost-effective and higher throughput\n        with table.batch_writer() as batch:\n            for item in items:\n                batch.put_item(Item=item)\n```\n\n## Key Considerations for Optimization\n\n1. **Consistent Item Sizes**: Keep item sizes consistent to avoid throughput variations\n2. **Parallel Processing**: Use parallel processing for large datasets\n3. **Adaptive Batch Sizing**: Adjust batch sizes based on throttling feedback\n4. **Sparse Indexes**: Use sparse indexes to reduce write costs\n5. **Monitoring**: Track consumed capacity to optimize cost and performance\n6. **Error Handling**: Implement robust retry logic with exponential backoff\n7. **Choose Wisely**: Select batch operations for throughput, transactions for atomicity\n8. **Compression**: Consider compressing large attribute values before storing\n\nBy implementing these optimizations, you can significantly improve the performance and cost-efficiency of your DynamoDB batch operations."
    }
  ]
}